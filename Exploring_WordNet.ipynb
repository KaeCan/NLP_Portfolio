{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuO2vtQEiEhr"
   },
   "source": [
    "# WordNet\n",
    "\n",
    "WordNet is a database full of semantic relations between words of many languages. Similar-meaning words are grouped together in *synsets*, which can be used to determine definitions, use-cases, lemmas.\n",
    "\n",
    "NLTK offers a Python interface to WordNet which can be used once we import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v4dzWXpnjlMn",
    "outputId": "2306116c-22b6-481b-d8d5-c893a87badba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('book')\n",
    "\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LMMNDIBjq4P"
   },
   "source": [
    "## Nouns\n",
    "\n",
    "As an example, I select a noun and output all of its synsets. The output will be a list of synsets that are relevant to the selected word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QuUXYp0Ujzg_",
    "outputId": "5e68f949-edc9-41b0-a3b8-bcb48d58c4dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('friend.n.01'),\n",
       " Synset('ally.n.02'),\n",
       " Synset('acquaintance.n.03'),\n",
       " Synset('supporter.n.01'),\n",
       " Synset('friend.n.05')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('friend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxce9z--kMgK"
   },
   "source": [
    "Let's extract its definition, usage examples, and lemmas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "NmRsbPnJkuY0",
    "outputId": "8d9449b9-8196-4536-dc64-e0404f157968"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'a person you know well and regard with affection and trust'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Definition\n",
    "wn.synset('friend.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZTxmwnIlLmj",
    "outputId": "c6ddd969-db86-4f96-b75e-a7ec88d5b85c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he was my best friend at the university']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Usage examples\n",
    "wn.synset('friend.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t9BTj3_blRPS",
    "outputId": "90986577-b0bf-43b5-f358-e970f9ff077c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('friend.n.01.friend')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmas\n",
    "wn.synset('friend.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks1i8qoTik3Q"
   },
   "source": [
    "We can also find the hypernyms of our word. Here, I demonstrate how to view the entire hierarchy of my chosen word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvo6m_Ovlcro",
    "outputId": "9f54f288-d4c4-469c-d8e3-c344d9c72a65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('person.n.01')\n",
      "Synset('causal_agent.n.01')\n",
      "Synset('physical_entity.n.01')\n",
      "Synset('entity.n.01')\n"
     ]
    }
   ],
   "source": [
    "#Traversing the hierarchy for a noun\n",
    "hyper = wn.synset('friend.n.01').hypernyms()[0]\n",
    "top = wn.synset('entity.n.01')\n",
    "\n",
    "while hyper:\n",
    "  print(hyper)\n",
    "  if hyper == top:\n",
    "    break\n",
    "  if hyper.hypernyms():\n",
    "    hyper = hyper.hypernyms()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5vbGwlBhl4J"
   },
   "source": [
    "You can see how the hierarchy is structure in a way that categorizes each noun as being a subclass of its parent. This pattern continues up until we reach the 'entity' noun, which encompasses all nouns.\n",
    "\n",
    "Below, I've provided ways to obtain hierarchical relations. Where there are no applicable words, there will be an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fSOkbrojFg4",
    "outputId": "f256b472-c0d3-481f-cc73-c8ce674104e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypernyms: [Synset('person.n.01')]\n",
      "Hyponyms: [Synset('alter_ego.n.01'), Synset('amigo.n.01'), Synset('best_friend.n.01'), Synset('brother.n.04'), Synset('buddy.n.01'), Synset('companion.n.01'), Synset('confidant.n.01'), Synset('flatmate.n.01'), Synset('girlfriend.n.01'), Synset('light.n.08'), Synset('mate.n.08'), Synset('roommate.n.01'), Synset('schoolfriend.n.01')]\n",
      "Meronyms: [] []\n",
      "Holonyms: []\n",
      "Antonyms: []\n"
     ]
    }
   ],
   "source": [
    "#Hypernyms\n",
    "print('Hypernyms:', wn.synset('friend.n.01').hypernyms())\n",
    "\n",
    "#Hyponyms\n",
    "print('Hyponyms:', wn.synset('friend.n.01').hyponyms())\n",
    "\n",
    "#Meronyms\n",
    "print('Meronyms:', wn.synset('friend.n.01').part_meronyms(), wn.synset('friend.n.01').substance_meronyms())\n",
    "\n",
    "#Holonyms\n",
    "print('Holonyms:', wn.synset('friend.n.01').member_holonyms())\n",
    "\n",
    "#Antonyms\n",
    "print('Antonyms:', wn.synset('friend.n.01').lemmas()[0].antonyms())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7BBxqoKlaQZ"
   },
   "source": [
    "## Verbs\n",
    "\n",
    "Now let's see what we can do with verbs. Below, I output a chosen word's synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_dKzzyvmG89",
    "outputId": "bcf9078c-c4aa-43c6-f78e-dc7f958c9689"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('punch.n.01'),\n",
       " Synset('punch.n.02'),\n",
       " Synset('punch.n.03'),\n",
       " Synset('punch.v.01'),\n",
       " Synset('punch.v.02'),\n",
       " Synset('punch.v.03')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('punch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzYDbKBImWm8"
   },
   "source": [
    "Now, I select a synset of a verb variation of the word and extract its definition, usage examples, and lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "kpXOhE74mfMB",
    "outputId": "a9e69960-a73f-4715-fe9f-d76647774b23"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'deliver a quick blow to'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Definition\n",
    "wn.synset('punch.v.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D_1JTR8Bmtbk",
    "outputId": "0f0608da-e92d-4054-966b-511ecc7c165b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he punched me in the stomach']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Usage examples\n",
    "wn.synset('punch.v.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fClCwjRPmysf",
    "outputId": "455f1512-0864-45a5-b4f4-2241ca440ca1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('punch.v.01.punch'), Lemma('punch.v.01.plug')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmas\n",
    "wn.synset('punch.v.01').lemmas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7n8kR6wm3qw"
   },
   "source": [
    "Here, I traverse up the hierarchy of my chosen word, outputting all synsets as I go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bjXKNzrnRUz",
    "outputId": "53952e4c-e363-4346-e623-52ee44f3a0dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('hit.v.03'), Synset('touch.v.01')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punch = wn.synset('punch.v.01')\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(punch.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CS0v6fumn9mc"
   },
   "source": [
    "You can see how nouns and verbs differ in that there is no common hypernym for all verbs. Each word ends its hierarchy in different places, unlike nouns.\n",
    "\n",
    "## Morphy\n",
    "\n",
    "The morphy() function will return the base form of a given word. When given an inflected word (and optionally the pos), morphy() will use English rules-based determination to find the root word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpcTw6eZopLj",
    "outputId": "9cc28376-9fbf-4afa-d534-ca7d5c60135f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punch\n",
      "None\n",
      "punch\n"
     ]
    }
   ],
   "source": [
    "print(wn.morphy('punching'))\n",
    "print(wn.morphy('punched', wn.ADV))\n",
    "print(wn.morphy('punched', wn.VERB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyHyLsO_pCNG"
   },
   "source": [
    "## Wu-Palmer Similarity Metric and the Lesk Algorithm\n",
    "\n",
    "A similarity metric is often used to determine how closely related two words are in terms of their usage in a language. A score is given in the range of 0 (little similarity) to 1 (identity).\n",
    "\n",
    "The Wu-Palmer similiarity metric is based on two words and their most specific common ancestor node.\n",
    "\n",
    "Below, I choose two words I believe may be similar to some degree to demonstrate this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lttVJy_4p9SF",
    "outputId": "635a17be-9bbf-43f4-d595-dc2cb1a4e343"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.wup_similarity(wn.synset('dog.n.01'), wn.synset('cat.n.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4nR4UWTqSX9"
   },
   "source": [
    "The Lesk algorithm returns the synset with the highest number of overlapping words between a given context sentence and definitions in each synset for a given word. We can additionally provide a pos argument for the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae3SIPR-qnEm",
    "outputId": "64680a2a-0832-4037-ae14-af6494af4670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('hit.n.01') (baseball) a successful stroke in an athletic contest (especially in baseball)\n",
      "Synset('hit.n.02') the act of contacting one thing with another\n",
      "Synset('hit.n.03') a conspicuous success\n",
      "Synset('collision.n.01') (physics) a brief event in which two or more bodies come together\n",
      "Synset('hit.n.05') a dose of a narcotic drug\n",
      "Synset('hit.n.06') a murder carried out by an underworld syndicate\n",
      "Synset('hit.n.07') a connection made via the internet to another website\n",
      "Synset('hit.v.01') cause to move by striking\n",
      "Synset('hit.v.02') hit against; come into sudden contact with\n",
      "Synset('hit.v.03') deal a blow to, either with the hand or with an instrument\n",
      "Synset('reach.v.01') reach a destination, either real or abstract\n",
      "Synset('hit.v.05') affect or afflict suddenly, usually adversely\n",
      "Synset('shoot.v.01') hit with a missile from a weapon\n",
      "Synset('stumble.v.03') encounter by chance\n",
      "Synset('score.v.01') gain points in a game\n",
      "Synset('hit.v.09') cause to experience suddenly\n",
      "Synset('strike.v.04') make a strategic, offensive, assault against an enemy, opponent, or a target\n",
      "Synset('murder.v.01') kill intentionally and with premeditation\n",
      "Synset('hit.v.12') drive something violently into a location\n",
      "Synset('reach.v.02') reach a point in time, or a certain state or level\n",
      "Synset('strike.v.10') produce by manipulating keys or strings of musical instruments, also metaphorically\n",
      "Synset('hit.v.15') consume to excess\n",
      "Synset('hit.v.16') hit the intended target or goal\n",
      "Synset('hit.v.17') pay unsolicited and usually unwanted sexual attention to\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "for ss in wn.synsets('hit'):\n",
    "  print(ss, ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5z9ORLjq4p-",
    "outputId": "a02826a3-7a87-46cb-e39e-318eb38790df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('hit.n.05')\n",
      "Synset('shoot.v.01')\n"
     ]
    }
   ],
   "source": [
    "my_sentence = ['Let', 'me', 'take', 'a', 'hit', 'of', 'that', '.']\n",
    "\n",
    "print(lesk(my_sentence, 'hit', 'n'))\n",
    "print(lesk(my_sentence, 'hit'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwIc0rOhrf2O"
   },
   "source": [
    "Specifying the pos will greatly help determining the correct synset of which a context-sentence's definition comes from. Notice how without the help of the target pos, the algorithm incorrectly returned a synset with a definition that has nothing to do with my sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peggAar4sKvA"
   },
   "source": [
    "## SentiWordNet\n",
    "\n",
    "SentiWordNet is designed for opinion mining. In other words, it is used to give sentiment scores for a given synset: positivity, negativity, objectivity. Each value is always in the range of 0 and 1, with all three scores adding up to the sum of 1.0.\n",
    "\n",
    "Below, I select an emotionally charged word, and output the polarity scores for each of its synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNJbs2G2sGZT",
    "outputId": "7bc3b203-ae80-4772-8c21-f96dd2c6467b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<anticipation.n.01: PosScore=0.125 NegScore=0.25>\n",
      "Positive score:  0.125\n",
      "Negative score:  0.25\n",
      "Objective score:  0.625 \n",
      "\n",
      "<anticipation.n.02: PosScore=0.0 NegScore=0.0>\n",
      "Positive score:  0.0\n",
      "Negative score:  0.0\n",
      "Objective score:  1.0 \n",
      "\n",
      "<prediction.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Positive score:  0.0\n",
      "Negative score:  0.0\n",
      "Objective score:  1.0 \n",
      "\n",
      "<anticipation.n.04: PosScore=0.5 NegScore=0.0>\n",
      "Positive score:  0.5\n",
      "Negative score:  0.0\n",
      "Objective score:  0.5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "for ss in list(swn.senti_synsets('anticipation')):\n",
    "  anti = ss\n",
    "  print(anti)\n",
    "  print('Positive score: ', anti.pos_score())\n",
    "  print('Negative score: ', anti.neg_score())\n",
    "  print('Objective score: ', anti.obj_score(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzv1cC3ivLaS"
   },
   "source": [
    "Now, let's try extracting polarity scores for each word in a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZxXBT3fYvl71",
    "outputId": "0c3adccd-e608-49e0-f02d-77fa6c42c448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anticipate', 'lots', 'of', 'homework', 'this', 'semester']\n",
      "<expect.v.01: PosScore=0.25 NegScore=0.25>\n",
      "Positive score:  0.25\n",
      "Negative score:  0.25\n",
      "Objective score:  0.5 \n",
      "\n",
      "<tons.n.01: PosScore=0.0 NegScore=0.25>\n",
      "Positive score:  0.0\n",
      "Negative score:  0.25\n",
      "Objective score:  0.75 \n",
      "\n",
      "<homework.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Positive score:  0.0\n",
      "Negative score:  0.0\n",
      "Objective score:  1.0 \n",
      "\n",
      "<semester.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Positive score:  0.0\n",
      "Negative score:  0.0\n",
      "Objective score:  1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_sentence = 'anticipate lots of homework this semester'\n",
    "neg = 0\n",
    "pos = 0\n",
    "words = my_sentence.split()\n",
    "print(words)\n",
    "for word in words:\n",
    "  syn_list = list(swn.senti_synsets(word))\n",
    "  if syn_list:\n",
    "    syn = syn_list[0]\n",
    "    print(syn)\n",
    "    print('Positive score: ', syn.pos_score())\n",
    "    print('Negative score: ', syn.neg_score())\n",
    "    print('Objective score: ', syn.obj_score(), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rd3vWmWSxxUO"
   },
   "source": [
    "Notice that each significant word returns their polarity scores. SentiWordNet does not categorize stopwords as well, so those are ignored. This will be really good in use for future NLP applications that requires a program to analyze the sentiment behind a sentence. For example, a program may need to know if you feel happy about something, or if you find something unfavorable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcaG006XygZP"
   },
   "source": [
    "## Collocation\n",
    "Collocation is the natural juxtaposition of two or more words that form a greater meaning than the sum of its parts beyond mere coincidence. For example, 'fast food' means more than what the words individually can convey.\n",
    "\n",
    "I will to use the Inaugural corpus in NLTK, so I will import it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYJJJw2746Yc",
    "outputId": "eb3b6f3f-3a54-450d-c9af-e17ba8893b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States; fellow citizens; years ago; four years; Federal\n",
      "Government; General Government; American people; Vice President; God\n",
      "bless; Chief Justice; one another; fellow Americans; Old World;\n",
      "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
      "tribes; public debt; foreign nations\n"
     ]
    }
   ],
   "source": [
    "#Get collocations\n",
    "from nltk.corpus import *\n",
    "text4.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC0ZCjb85Ovl"
   },
   "source": [
    "Let's select one of the collocations identified above and calculate mutual information.\n",
    "\n",
    "Mutual information is the log of the probability:\n",
    "\n",
    "P(x,y) / [P(x) * P(y)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_xlMVRo5VUl",
    "outputId": "be247736-041a-440f-f3b1-63f27852e932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p('fellow'):  0.013665835411471322\n",
      "p('citizens'): 0.026932668329177057\n",
      "p('fellow citizens'): 0.006084788029925187\n",
      "Mutual information score:  4.0472042737811735\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "text = ' '.join(text4.tokens)\n",
    "\n",
    "vocab = len(set(text4))\n",
    "fellow = text.count('fellow')/vocab\n",
    "print('p(\\'fellow\\'): ', fellow)\n",
    "citizens = text.count('citizens')/vocab\n",
    "print('p(\\'citizens\\'):', citizens)\n",
    "fellow_citizens = text.count('fellow citizens')/vocab\n",
    "print('p(\\'fellow citizens\\'):', fellow_citizens)\n",
    "\n",
    "pmi = math.log2(fellow_citizens / (fellow * citizens))\n",
    "\n",
    "print('Mutual information score: ', pmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9p9sWzn7jLC"
   },
   "source": [
    "The number outputted for the MI score calculation is used to measure the amount of non-randomness present when the two words occur in text. Mutual information is vital in assessing how important collocation is in a given text, and suggest that the target words may exist in the form of a two-way attraction."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
