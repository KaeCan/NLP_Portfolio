{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Author Attribution with SKLearn\n",
        "*The Federalist Papers* is a collection of documents written by Alexander Hamilton, James Madison, and John Jay collectively under the pseudonym Publius. These documents were written to persuade voters to ratify the US Constitution. These documents continue to be influential to this day, as they are frequently cited in Federal court rulings, as well as law blogs, and political opinions.\n",
        "\n",
        "Here, we use *The Federlist Papers* as means to demonstrate NLP authorship attribution, as in the attempt to identify the author of a document, given samples of the authors' work.\n",
        "\n",
        "### Setting up\n",
        "\n",
        "First, I use pandas' built in method to read in a csv file from my GitHub.\n",
        "I have also displayed the first few rows of data."
      ],
      "metadata": {
        "id": "f38aC8rsRSC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/KaeCan/NLP_Portfolio/main/SKLearn/federalist.csv\")\n",
        "df['author'] = pd.Categorical(df['author'])\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4F8x-usyRJNj",
        "outputId": "374fb44d-f130-4d27-e950-818766680f2b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     author                                               text\n",
              "0  HAMILTON  FEDERALIST. No. 1 General Introduction For the...\n",
              "1       JAY  FEDERALIST No. 2 Concerning Dangers from Forei...\n",
              "2       JAY  FEDERALIST No. 3 The Same Subject Continued (C...\n",
              "3       JAY  FEDERALIST No. 4 The Same Subject Continued (C...\n",
              "4       JAY  FEDERALIST No. 5 The Same Subject Continued (C..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7051d0a-e0d6-4c12-a972-637310443289\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>HAMILTON</td>\n",
              "      <td>FEDERALIST. No. 1 General Introduction For the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>JAY</td>\n",
              "      <td>FEDERALIST No. 2 Concerning Dangers from Forei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>JAY</td>\n",
              "      <td>FEDERALIST No. 3 The Same Subject Continued (C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>JAY</td>\n",
              "      <td>FEDERALIST No. 4 The Same Subject Continued (C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>JAY</td>\n",
              "      <td>FEDERALIST No. 5 The Same Subject Continued (C...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7051d0a-e0d6-4c12-a972-637310443289')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e7051d0a-e0d6-4c12-a972-637310443289 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e7051d0a-e0d6-4c12-a972-637310443289');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use pandas to see how many papers each author has or potential has written."
      ],
      "metadata": {
        "id": "azN-3paJV-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(['author'])['author'].count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiszXx5dWLds",
        "outputId": "c5422a6a-8336-428d-f783-1f63a793a29a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "author\n",
              "HAMILTON                49\n",
              "HAMILTON AND MADISON     3\n",
              "HAMILTON OR MADISON     11\n",
              "JAY                      5\n",
              "MADISON                 15\n",
              "Name: author, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use sklearn's train and test split to divide our data into 80% train and 20% test. I outputted the shapes of each below."
      ],
      "metadata": {
        "id": "up70Smo48LfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.text\n",
        "Y = df.author\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, train_size=0.8, random_state=1234)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14sMngwSa3Z7",
        "outputId": "5fa8481f-f50c-4adb-b05b-2e34adc07ad5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(66,)\n",
            "(17,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can further refine our text by preprocessing the words. I have removed stop words and performed tf-idf vectorization. I have re-outputted the shapes of the training and test set."
      ],
      "metadata": {
        "id": "V1-GdP3j8hyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM6u30P8pkkQ",
        "outputId": "cfcf1db8-2b2f-47e1-fbd5-e74d40900b8f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(66, 7876)\n",
            "(17, 7876)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bernoulli Naive Bayes Model\n",
        "The first model I use is the Bernoulli Naive Bayes model. Each model I will output the model's prediction accuracy."
      ],
      "metadata": {
        "id": "M-LMgaip87Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "naive_bayes = BernoulliNB()\n",
        "naive_bayes.fit(X_train, Y_train)\n",
        "\n",
        "prediction = naive_bayes.predict(X_test)\n",
        "print('accuracy: ', accuracy_score(Y_test, prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NpTR3aPvZKR",
        "outputId": "b8f3dc0b-45f6-4a0b-981f-e8a2877aec83"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  0.5882352941176471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model gives us a disappointing 59% accuracy. I understand there are 7876 unique words in the vocabulary. This may be too much, and many of those words may not be helpful. I attempt to rectify this by further refining our vectorization to include only the 1000 most frequent words and add bigrams as a feature."
      ],
      "metadata": {
        "id": "auJmlTyI9PuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words=stopwords,\n",
        "    max_features=1000,\n",
        "    ngram_range=(1,2)\n",
        ")\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, random_state=1234)\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "naive_bayes2 = BernoulliNB()\n",
        "naive_bayes2.fit(X_train, Y_train)\n",
        "\n",
        "prediction = naive_bayes2.predict(X_test)\n",
        "print('accuracy: ', accuracy_score(Y_test, prediction))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57w2ArmB2WGk",
        "outputId": "b8909749-d0b7-4c3d-8d7c-d765eddc066f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  0.9411764705882353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, our accuracy has drastically improved from the refinements we've made to our vectorization.\n",
        "\n",
        "## Logistic Regression Model\n",
        "Next, I try to use logistic regression to fit the data. I began with a control model that has no parameters."
      ],
      "metadata": {
        "id": "TnS7TbvR928J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, Y_train)\n",
        "\n",
        "prediction = log_reg.predict(X_test)\n",
        "print('accuracy: ', accuracy_score(Y_test, prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHwGyd3b3m8Q",
        "outputId": "9e40a8bc-bfc5-4eb1-90b9-488ceb59f404"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  0.5882352941176471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, our accuracy is disappointing. However, we can adjust parameters to see if we can improve the accuracy."
      ],
      "metadata": {
        "id": "_NCC1IH3-l9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg2 = LogisticRegression(class_weight='balanced', C=10000)\n",
        "log_reg2.fit(X_train, Y_train)\n",
        "\n",
        "prediction = log_reg2.predict(X_test)\n",
        "print('accuracy: ', accuracy_score(Y_test, prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEque7Ml5ZB4",
        "outputId": "ed316025-f1a8-4719-a856-e90d854bef5a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  0.8235294117647058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By tinkering with C (regularization strength) and balancing the data, I was able to improve the model's accuracy by a significant degree.\n",
        "\n",
        "## Neural Networks\n",
        "Finally, I will attempt to train using a neural network."
      ],
      "metadata": {
        "id": "n32RXCby-w-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "neural = MLPClassifier(hidden_layer_sizes=(10,8), max_iter=10000)\n",
        "neural.fit(X_train, Y_train)\n",
        "\n",
        "prediction = neural.predict(X_test)\n",
        "print('accuracy: ', accuracy_score(Y_test, prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sivo_XdE50Iw",
        "outputId": "de3f58a1-e57e-4cea-956e-a4f25f058211"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  0.8823529411764706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It took a few trials, but I've managed to obtain a final accuracy of 88% by tinkering with different topologies. "
      ],
      "metadata": {
        "id": "NOb28zZR_TGj"
      }
    }
  ]
}