{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing NLTK\n",
        "\n",
        "NLTK requires the user to install the modules they need. All modules can be installed using 'all', or you can install them individually."
      ],
      "metadata": {
        "id": "o0jQdhi5zKaA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KSuINww2y6M_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3900e7-9ae2-48d8-ee3c-4d517653fec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('book')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#Using NLTK Text objects\n",
        "\n",
        "Each of the built-in 9 texts is an NLTK Text object. Documentation can be found [here](https://www.nltk.org/_modules/nltk/text.html). Each Text object can be analyzed using various methods.\n",
        "\n",
        "Here, we are printing the first 20 tokens of 'text1'."
      ],
      "metadata": {
        "id": "j-iaLsIgzJME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import text1\n",
        "\n",
        "list = []\n",
        "list = text1.tokens\n",
        "\n",
        "for x in range(0,20):\n",
        "  print(list[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijq7kcki01MU",
        "outputId": "a6ab47cc-1cf0-4f3c-aa82-0a4bb68c2d03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
            "[\n",
            "Moby\n",
            "Dick\n",
            "by\n",
            "Herman\n",
            "Melville\n",
            "1851\n",
            "]\n",
            "ETYMOLOGY\n",
            ".\n",
            "(\n",
            "Supplied\n",
            "by\n",
            "a\n",
            "Late\n",
            "Consumptive\n",
            "Usher\n",
            "to\n",
            "a\n",
            "Grammar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The tokens() method and Text objects\n",
        "A couple things I've learned:\n",
        "1. Text objects can have their words' locations revealed to construct lexical dispersion plots that show where and how often the word appears in the text\n",
        "2. Text objects also support counting the number of times a certain word appears in the text\n",
        "\n",
        "###Searching for certain words\n",
        "\n",
        "Using the concordance() method, we can look up where a particular word appears and even select how many lines/instances to display."
      ],
      "metadata": {
        "id": "SHMTh-rG3tk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1.concordance(\"sea\", lines=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEolDVS946Xn",
        "outputId": "5d5697a0-16dc-4155-f100-806e5aa4fde8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 5 of 455 matches:\n",
            " shall slay the dragon that is in the sea .\" -- ISAIAH \" And what thing soever \n",
            " S PLUTARCH ' S MORALS . \" The Indian Sea breedeth the most and the biggest fis\n",
            "cely had we proceeded two days on the sea , when about sunrise a great many Wha\n",
            "many Whales and other monsters of the sea , appeared . Among the former , one w\n",
            " waves on all sides , and beating the sea before him into a foam .\" -- TOOKE ' \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NLTK's and Python's count() method\n",
        "1. Python's count() works by returning the number of times an object appears in a **list**\n",
        "2. NLTK's count() works by returning the number of times a word appears in a **text object**\n",
        "\n",
        "##Tokenizing Text\n",
        "\n",
        "I will be using an excerpt from *What We Know About Acquisition of Adult Literacy* as an example.\n",
        "\n",
        "Copy this text if you'd like:\n",
        "\"*Imagine for a moment that among humans some people can fly. Government staff come and tell you that you can take a course that will teach you how. This sounds great, and one hears of emotional accounts of what it is like to soar in the sky. But you have no personal experience of what flying feels like. To learn it you must go for six to nine months daily to school. You do exercises like flapping your arms but you never really take off. And you do not often need to fly anywhere. Whenever you do, you can either take the plane or send a relative who can fly to do what is needed. So, is the benefit worth the effort?*\"\n",
        "\n",
        "I use NLTK's word tokenizer word_tokenize() to split the text into tokens. Here, I am printing out the first 10 words of the excerpt.\n"
      ],
      "metadata": {
        "id": "tIiQtdK-5z1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "raw_text = \"Imagine for a moment that among humans some people can fly. Government staff come and tell you that you can take a course that will teach you how. This sounds great, and one hears of emotional accounts of what it is like to soar in the sky. But you have no personal experience of what flying feels like. To learn it you must go for six to nine months daily to school. You do exercises like flapping your arms but you never really take off. And you do not often need to fly anywhere. Whenever you do, you can either take the plane or send a relative who can fly to do what is needed. So, is the benefit worth the effort?\"\n",
        "tokens = word_tokenize(raw_text)\n",
        "\n",
        "for x in range(0,10):\n",
        "  print(tokens[x])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EuQN0Pi6RgT",
        "outputId": "0d90abe0-f442-4bf8-8db1-ed9b86454f1a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine\n",
            "for\n",
            "a\n",
            "moment\n",
            "that\n",
            "among\n",
            "humans\n",
            "some\n",
            "people\n",
            "can\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK's sentence tokenizer sent_tokenize() will perform sentence segmentation similar to word_tokenize() in that it is splitting the text. Here, I am simply displaying all the sentences."
      ],
      "metadata": {
        "id": "wBqpSYXy9pNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sents = sent_tokenize(raw_text)\n",
        "\n",
        "for x in sents:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9lyY03N-H23",
        "outputId": "1bb2bbb7-eeef-4108-da83-f5b39fbd17e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine for a moment that among humans some people can fly.\n",
            "Government staff come and tell you that you can take a course that will teach you how.\n",
            "This sounds great, and one hears of emotional accounts of what it is like to soar in the sky.\n",
            "But you have no personal experience of what flying feels like.\n",
            "To learn it you must go for six to nine months daily to school.\n",
            "You do exercises like flapping your arms but you never really take off.\n",
            "And you do not often need to fly anywhere.\n",
            "Whenever you do, you can either take the plane or send a relative who can fly to do what is needed.\n",
            "So, is the benefit worth the effort?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming and Lemmatization\n",
        "There are too many words in the English language for a program to efficiently process. That's why we must 'stem' or 'lemmatize' words in order to reduce the number of words a program has to analyze.\n",
        "\n",
        "Below, we use NLTK's PorterStemmer() to stem the text and display the list."
      ],
      "metadata": {
        "id": "DNpH7qHskcmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "tokens = word_tokenize(raw_text)\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "print(stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbjNrfBHlHYz",
        "outputId": "8b69fce0-eeae-4174-f9bb-3763ffab97c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['imagin', 'for', 'a', 'moment', 'that', 'among', 'human', 'some', 'peopl', 'can', 'fli', '.', 'govern', 'staff', 'come', 'and', 'tell', 'you', 'that', 'you', 'can', 'take', 'a', 'cours', 'that', 'will', 'teach', 'you', 'how', '.', 'thi', 'sound', 'great', ',', 'and', 'one', 'hear', 'of', 'emot', 'account', 'of', 'what', 'it', 'is', 'like', 'to', 'soar', 'in', 'the', 'sky', '.', 'but', 'you', 'have', 'no', 'person', 'experi', 'of', 'what', 'fli', 'feel', 'like', '.', 'to', 'learn', 'it', 'you', 'must', 'go', 'for', 'six', 'to', 'nine', 'month', 'daili', 'to', 'school', '.', 'you', 'do', 'exercis', 'like', 'flap', 'your', 'arm', 'but', 'you', 'never', 'realli', 'take', 'off', '.', 'and', 'you', 'do', 'not', 'often', 'need', 'to', 'fli', 'anywher', '.', 'whenev', 'you', 'do', ',', 'you', 'can', 'either', 'take', 'the', 'plane', 'or', 'send', 'a', 'rel', 'who', 'can', 'fli', 'to', 'do', 'what', 'is', 'need', '.', 'so', ',', 'is', 'the', 'benefit', 'worth', 'the', 'effort', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using, NLTK's WordNetLemmatizer(), we can lemmatize the excerpt from before."
      ],
      "metadata": {
        "id": "zT6nKp2clkB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens = word_tokenize(raw_text)\n",
        "lemms = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(lemms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXtZz_9llyJi",
        "outputId": "c623cfbc-a5f4-4866-f76d-1d90925664b4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Imagine', 'for', 'a', 'moment', 'that', 'among', 'human', 'some', 'people', 'can', 'fly', '.', 'Government', 'staff', 'come', 'and', 'tell', 'you', 'that', 'you', 'can', 'take', 'a', 'course', 'that', 'will', 'teach', 'you', 'how', '.', 'This', 'sound', 'great', ',', 'and', 'one', 'hears', 'of', 'emotional', 'account', 'of', 'what', 'it', 'is', 'like', 'to', 'soar', 'in', 'the', 'sky', '.', 'But', 'you', 'have', 'no', 'personal', 'experience', 'of', 'what', 'flying', 'feel', 'like', '.', 'To', 'learn', 'it', 'you', 'must', 'go', 'for', 'six', 'to', 'nine', 'month', 'daily', 'to', 'school', '.', 'You', 'do', 'exercise', 'like', 'flapping', 'your', 'arm', 'but', 'you', 'never', 'really', 'take', 'off', '.', 'And', 'you', 'do', 'not', 'often', 'need', 'to', 'fly', 'anywhere', '.', 'Whenever', 'you', 'do', ',', 'you', 'can', 'either', 'take', 'the', 'plane', 'or', 'send', 'a', 'relative', 'who', 'can', 'fly', 'to', 'do', 'what', 'is', 'needed', '.', 'So', ',', 'is', 'the', 'benefit', 'worth', 'the', 'effort', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Differences between stemming and lemmatization\n",
        "- Stem - Lemma\n",
        "- not always a word - always an actual word\n",
        "- faster - slower\n",
        "- does not use a corpus - uses a corpus\n",
        "- truncates words - reduces words using morphology\n",
        "- crude reduction - proper reduction\n"
      ],
      "metadata": {
        "id": "l6fL9-OTmCJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#My thoughts on NLTK\n",
        "\n",
        "1. Functionality\n",
        "  - Skimming the documentation (linked above), I can see a great amount of functionality in the API. NLTK has dozens of modules that can vary in purpose from testing, parsing, classifying, tokenizing, and analyzing text.\n",
        "2. Code Quality\n",
        "  - The documentation is very thorough and detailed. Every class and method is explained in their purpose and parameters are intuitive in understanding their usage.\n",
        "3. Potential\n",
        "  - There are many interesting ways to use NLTK and its many modules. A few examples are: in the making of a smart-assistant, service automation, and predictive text messaging. "
      ],
      "metadata": {
        "id": "akUZF2wioVpf"
      }
    }
  ]
}