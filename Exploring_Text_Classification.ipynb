{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "We used Kaggle data sets to experiement with text classification. The data set we've chosen classifies tweets based on their sentiment related to COVID-19 and the pandemic. Sentiment varies from extremely positive to extremely negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing what data we're working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Extremely Negative    2464\n",
       "Extremely Positive    2423\n",
       "Negative              4014\n",
       "Neutral               2813\n",
       "Positive              4286\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_csv('Kaggle_Data/Corona_NLP_test.csv', header=0, usecols=[4,5], encoding='latin-1')\n",
    "data_train = pd.read_csv('Kaggle_Data/Corona_NLP_train.csv', header=0, usecols=[4,5], encoding='latin-1')[:16000]\n",
    "\n",
    "data_test.groupby(['Sentiment'])['Sentiment'].count()\n",
    "data_train.groupby(['Sentiment'])['Sentiment'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shapes: (16000, 25000) (16000,)\n",
      "test shapes: (3798, 25000) (3798,)\n",
      "test first five labels: [0 4 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# set up X and Y\n",
    "num_labels = 5\n",
    "vocab_size = 25000\n",
    "batch_size = 100\n",
    "\n",
    "# fit the tokenizer on the training data\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data_train.OriginalTweet)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(data_train.OriginalTweet, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(data_test.OriginalTweet, mode='tfidf')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(data_train.Sentiment)\n",
    "\n",
    "y_train = encoder.transform(data_train.Sentiment)\n",
    "y_test = encoder.transform(data_test.Sentiment)\n",
    "\n",
    "# check shape\n",
    "print(\"train shapes:\", x_train.shape, y_train.shape)\n",
    "print(\"test shapes:\", x_test.shape, y_test.shape)\n",
    "print(\"test first five labels:\", y_test[:5])\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=None)\n",
    "y_test = to_categorical(y_test, num_classes=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "144/144 [==============================] - 3s 16ms/step - loss: 1.5765 - accuracy: 0.2746 - val_loss: 1.5222 - val_accuracy: 0.3081\n",
      "Epoch 2/10\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 1.3768 - accuracy: 0.3884 - val_loss: 1.2700 - val_accuracy: 0.4412\n",
      "Epoch 3/10\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 1.1006 - accuracy: 0.5374 - val_loss: 1.1678 - val_accuracy: 0.5163\n",
      "Epoch 4/10\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 0.8991 - accuracy: 0.6423 - val_loss: 1.1476 - val_accuracy: 0.5344\n",
      "Epoch 5/10\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 0.7429 - accuracy: 0.7164 - val_loss: 1.1702 - val_accuracy: 0.5362\n",
      "Epoch 6/10\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 0.6260 - accuracy: 0.7686 - val_loss: 1.2459 - val_accuracy: 0.5544\n",
      "Epoch 7/10\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 0.5385 - accuracy: 0.8047 - val_loss: 1.3872 - val_accuracy: 0.5444\n",
      "Epoch 8/10\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 0.4694 - accuracy: 0.8381 - val_loss: 1.4966 - val_accuracy: 0.5462\n",
      "Epoch 9/10\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 0.4001 - accuracy: 0.8616 - val_loss: 1.5952 - val_accuracy: 0.5494\n",
      "Epoch 10/10\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 0.3692 - accuracy: 0.8735 - val_loss: 1.6815 - val_accuracy: 0.5431\n"
     ]
    }
   ],
   "source": [
    "seq_model = models.Sequential()\n",
    "seq_model.add(layers.Dense(24, input_dim=vocab_size, kernel_initializer='normal', activation='relu'))\n",
    "seq_model.add(layers.Dropout(.35))\n",
    "seq_model.add(layers.Dense(12, kernel_initializer='normal', activation='relu'))\n",
    "seq_model.add(layers.Dropout(.35))\n",
    "seq_model.add(layers.Dense(5, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "\n",
    "seq_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = seq_model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step - loss: 1.8928 - accuracy: 0.5250\n",
      "Accuracy:  0.5250131487846375\n"
     ]
    }
   ],
   "source": [
    "score = seq_model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU (RNN) Architecture\n",
    "\n",
    "We train a GRU and use one-hot encoding to vectorize the words of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_VOCAB_SIZE = 500\n",
    "\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(data_train.Sentiment)\n",
    "y_train = encoder.transform(data_train.Sentiment)\n",
    "y_test = encoder.transform(data_test.Sentiment)\n",
    "\n",
    "encoder = keras.layers.TextVectorization(max_tokens=LSTM_VOCAB_SIZE)\n",
    "encoder.adapt(data_train.OriginalTweet)\n",
    "\n",
    "x_train = encoder(data_train.OriginalTweet)\n",
    "x_test = encoder(data_test.OriginalTweet)\n",
    "\n",
    "x_train = tf.keras.utils.to_categorical(x_train, num_classes = LSTM_VOCAB_SIZE)\n",
    "x_test = tf.keras.utils.to_categorical(x_test, num_classes = LSTM_VOCAB_SIZE)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=None)\n",
    "y_test = to_categorical(y_test, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "450/450 [==============================] - 25s 46ms/step - loss: 1.5331 - accuracy: 0.2877 - val_loss: 1.4874 - val_accuracy: 0.3125\n",
      "Epoch 2/10\n",
      "450/450 [==============================] - 20s 45ms/step - loss: 1.3746 - accuracy: 0.3970 - val_loss: 1.2595 - val_accuracy: 0.4781\n",
      "Epoch 3/10\n",
      "450/450 [==============================] - 20s 45ms/step - loss: 1.2231 - accuracy: 0.4812 - val_loss: 1.2261 - val_accuracy: 0.4875\n",
      "Epoch 4/10\n",
      "450/450 [==============================] - 20s 45ms/step - loss: 1.1793 - accuracy: 0.5074 - val_loss: 1.2187 - val_accuracy: 0.4981\n",
      "Epoch 5/10\n",
      "450/450 [==============================] - 20s 45ms/step - loss: 1.1458 - accuracy: 0.5233 - val_loss: 1.2635 - val_accuracy: 0.4719\n",
      "Epoch 6/10\n",
      "450/450 [==============================] - 20s 45ms/step - loss: 1.1221 - accuracy: 0.5357 - val_loss: 1.2523 - val_accuracy: 0.4800\n",
      "Epoch 7/10\n",
      "450/450 [==============================] - 20s 45ms/step - loss: 1.0933 - accuracy: 0.5509 - val_loss: 1.2352 - val_accuracy: 0.4781\n",
      "Epoch 8/10\n",
      "450/450 [==============================] - 20s 46ms/step - loss: 1.0649 - accuracy: 0.5630 - val_loss: 1.2519 - val_accuracy: 0.4837\n",
      "Epoch 9/10\n",
      "450/450 [==============================] - 21s 46ms/step - loss: 1.0384 - accuracy: 0.5754 - val_loss: 1.2937 - val_accuracy: 0.4806\n",
      "Epoch 10/10\n",
      "450/450 [==============================] - 21s 46ms/step - loss: 1.0120 - accuracy: 0.5884 - val_loss: 1.3071 - val_accuracy: 0.4512\n"
     ]
    }
   ],
   "source": [
    "lstm = keras.Sequential()\n",
    "lstm.add(layers.Bidirectional(layers.GRU(48, input_dim=LSTM_VOCAB_SIZE)))\n",
    "lstm.add(layers.Dense(10, activation='relu'))\n",
    "lstm.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "lstm.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = lstm.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 11s 73ms/step - loss: 1.3269 - accuracy: 0.4539\n",
      "Accuracy:  0.4539231061935425\n"
     ]
    }
   ],
   "source": [
    "score = lstm.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying an embedding layer to our GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_VOCAB_SIZE = 10000\n",
    "\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(data_train.Sentiment)\n",
    "y_train = encoder.transform(data_train.Sentiment)\n",
    "y_test = encoder.transform(data_test.Sentiment)\n",
    "\n",
    "encoder = keras.layers.TextVectorization(max_tokens=LSTM_VOCAB_SIZE)\n",
    "encoder.adapt(data_train.OriginalTweet)\n",
    "\n",
    "x_train = encoder(data_train.OriginalTweet)\n",
    "x_test = encoder(data_test.OriginalTweet)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=None)\n",
    "y_test = to_categorical(y_test, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "450/450 [==============================] - 24s 45ms/step - loss: 1.3674 - accuracy: 0.3910 - val_loss: 1.0634 - val_accuracy: 0.5788\n",
      "Epoch 2/10\n",
      "450/450 [==============================] - 18s 39ms/step - loss: 0.8592 - accuracy: 0.6712 - val_loss: 0.8865 - val_accuracy: 0.6662\n",
      "Epoch 3/10\n",
      "450/450 [==============================] - 18s 39ms/step - loss: 0.6240 - accuracy: 0.7790 - val_loss: 0.8763 - val_accuracy: 0.6850\n",
      "Epoch 4/10\n",
      "450/450 [==============================] - 18s 39ms/step - loss: 0.4837 - accuracy: 0.8349 - val_loss: 0.9248 - val_accuracy: 0.6756\n",
      "Epoch 5/10\n",
      "450/450 [==============================] - 18s 40ms/step - loss: 0.3703 - accuracy: 0.8806 - val_loss: 1.0267 - val_accuracy: 0.6681\n",
      "Epoch 6/10\n",
      "450/450 [==============================] - 18s 39ms/step - loss: 0.2811 - accuracy: 0.9090 - val_loss: 1.1183 - val_accuracy: 0.6394\n",
      "Epoch 7/10\n",
      "450/450 [==============================] - 18s 39ms/step - loss: 0.2141 - accuracy: 0.9358 - val_loss: 1.2786 - val_accuracy: 0.6456\n",
      "Epoch 8/10\n",
      "450/450 [==============================] - 18s 39ms/step - loss: 0.1632 - accuracy: 0.9517 - val_loss: 1.4674 - val_accuracy: 0.6425\n",
      "Epoch 9/10\n",
      "450/450 [==============================] - 18s 40ms/step - loss: 0.1348 - accuracy: 0.9601 - val_loss: 1.5081 - val_accuracy: 0.6406\n",
      "Epoch 10/10\n",
      "450/450 [==============================] - 18s 40ms/step - loss: 0.1095 - accuracy: 0.9680 - val_loss: 1.6565 - val_accuracy: 0.6425\n"
     ]
    }
   ],
   "source": [
    "lstm = keras.Sequential()\n",
    "lstm.add(layers.Embedding(input_dim=LSTM_VOCAB_SIZE, output_dim=64))\n",
    "lstm.add(layers.Bidirectional(layers.GRU(64, input_dim=LSTM_VOCAB_SIZE)))\n",
    "lstm.add(layers.Dense(10, activation='relu'))\n",
    "lstm.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "lstm.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = lstm.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 2s 12ms/step - loss: 1.7975 - accuracy: 0.6222\n",
      "Accuracy:  0.622169554233551\n"
     ]
    }
   ],
   "source": [
    "score = lstm.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Accuracy: ', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cf0682b350d42b9408e5df7ea154928f0b001ef85c445fee0a76691c6202096"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
